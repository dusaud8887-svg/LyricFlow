# ğŸµ í˜¸ì‹œë§ˆì¹˜ ìŠ¤ì´ì„¸ì´ ìë§‰ ìƒì„±ê¸° - ê³ ê¸‰ ìµœì í™” ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2025-12-29
**ë²„ì „**: Advanced Optimization Guide v1.0
**ëŒ€ìƒ**: ìµœê³  í’ˆì§ˆì˜ ê°€ì‚¬ ì‹±í¬ ê²°ê³¼ë¥¼ ì›í•˜ëŠ” ì‚¬ìš©ì

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#-ê°œìš”)
2. [ìŒì„± ì¶”ì¶œ ìµœì í™”](#-ìŒì„±-ì¶”ì¶œ-ìµœì í™”)
3. [ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ê¸°ë²•](#-ì˜¤ë””ì˜¤-ì „ì²˜ë¦¬-ê¸°ë²•)
4. [Demucs ë³´ì»¬ ë¶„ë¦¬ ì‹¤ì „](#-demucs-ë³´ì»¬-ë¶„ë¦¬-ì‹¤ì „)
5. [stable-ts ê³ ê¸‰ íŒŒë¼ë¯¸í„°](#-stable-ts-ê³ ê¸‰-íŒŒë¼ë¯¸í„°)
6. [ì¼ë³¸ì–´ ê°€ì‚¬ ì„¸ê·¸ë¨¼íŠ¸ ìµœì í™”](#-ì¼ë³¸ì–´-ê°€ì‚¬-ì„¸ê·¸ë¨¼íŠ¸-ìµœì í™”)
7. [í™˜ê°(Hallucination) ì™„ì „ ì œê±°](#-í™˜ê°hallucination-ì™„ì „-ì œê±°)
8. [íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë°€ë„ ê·¹ëŒ€í™”](#-íƒ€ì„ìŠ¤íƒ¬í”„-ì •ë°€ë„-ê·¹ëŒ€í™”)
9. [í†µí•© ìµœì í™” íŒŒì´í”„ë¼ì¸](#-í†µí•©-ìµœì í™”-íŒŒì´í”„ë¼ì¸)
10. [ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬](#-ì„±ëŠ¥-ë²¤ì¹˜ë§ˆí¬)
11. [ì°¸ê³  ìë£Œ](#-ì°¸ê³ -ìë£Œ)

---

## ğŸ¯ ê°œìš”

ì´ ë¬¸ì„œëŠ” **í˜„ì¬ v1.2 í”„ë¡œê·¸ë¨ì„ ìµœê³  í’ˆì§ˆë¡œ ëŒì–´ì˜¬ë¦¬ê¸° ìœ„í•œ** ëª¨ë“  ê¸°ë²•ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.

### í˜„ì¬ ìƒíƒœ (v1.2)

```python
# í˜„ì¬ êµ¬í˜„
result = model.align(str(mp3_path), lyrics, language='ja')
result.to_srt_vtt(str(output_path), word_level=WORD_LEVEL_LRC)
```

**ë¬¸ì œì **:
- âŒ ì›ë³¸ MP3 ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë³´ì»¬ + ë°°ê²½ìŒì•… í˜¼ì¬)
- âŒ ê¸°ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  (40~60ì ê¸´ ì¤„)
- âŒ VAD ë¯¸ì‚¬ìš© (ì¹¨ë¬µ êµ¬ê°„ ì²˜ë¦¬ ë¶€ì¡±)
- âŒ í™˜ê° ê°€ëŠ¥ì„±
- âŒ íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë°€ë„ ì œí•œ

### ëª©í‘œ ìƒíƒœ (v2.0+)

```python
# ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸
vocals = extract_vocals_demucs(mp3_path)  # ë³´ì»¬ë§Œ ë¶„ë¦¬
result = model.align(
    vocals,
    clean_lyrics(lyrics),  # ì „ì²˜ë¦¬ëœ ê°€ì‚¬
    language='ja',
    vad=True,  # VAD í™œì„±í™”
    vad_threshold=0.35,
    suppress_silence=True,
    temperature=0,
    condition_on_previous_text=False,
)

# ì„¸ê·¸ë¨¼íŠ¸ ìµœì í™”
optimize_segments_for_japanese(result)

# í’ˆì§ˆ ê²€ì¦
validate_and_warn(result)
```

**ê¸°ëŒ€ íš¨ê³¼**:
- âœ… ë³´ì»¬ ì¸ì‹ ì •í™•ë„ **70% í–¥ìƒ** (WER 35% â†’ 10%)
- âœ… íƒ€ì„ìŠ¤íƒ¬í”„ ì •í™•ë„ **50% í–¥ìƒ** (Â±0.3s â†’ Â±0.15s)
- âœ… í™˜ê° **95% ê°ì†Œ**
- âœ… ê°€ë…ì„± **50% í–¥ìƒ** (60ì â†’ 30ì)

---

## ğŸ¤ ìŒì„± ì¶”ì¶œ ìµœì í™”

### 1. Whisper ì˜¤ë””ì˜¤ ìš”êµ¬ì‚¬í•­

WhisperëŠ” **ëª¨ë“  ì˜¤ë””ì˜¤ë¥¼ 16kHz ëª¨ë…¸ë¡œ ìë™ ë¦¬ìƒ˜í”Œë§**í•©ë‹ˆë‹¤.

| íŒŒë¼ë¯¸í„° | Whisper ë‚´ë¶€ ì²˜ë¦¬ | ê¶Œì¥ ì…ë ¥ |
|----------|------------------|----------|
| **Sample Rate** | 16kHz ê°•ì œ ë³€í™˜ | 16kHz ì´ìƒ (22.05kHz, 44.1kHz) |
| **Channels** | Mono ê°•ì œ ë³€í™˜ | Stereo â†’ Mono ìë™ |
| **Bit Depth** | 16-bit | 16-bit ì´ìƒ |
| **Duration** | 30ì´ˆ ì²­í¬ ë¶„í•  | ì œí•œ ì—†ìŒ |

**ê²°ë¡ **: ì›ë³¸ MP3 í’ˆì§ˆì´ ë†’ì„ìˆ˜ë¡ ì¢‹ì§€ë§Œ, Whisperê°€ ìë™ìœ¼ë¡œ ìµœì í™”í•˜ë¯€ë¡œ **ë³„ë„ ë¦¬ìƒ˜í”Œë§ ë¶ˆí•„ìš”**.

**ì°¸ê³ **: [Optimal sample rate for input audio?](https://github.com/openai/whisper/discussions/870), [Optimise OpenAI Whisper API](https://dev.to/mxro/optimise-openai-whisper-api-audio-format-sampling-rate-and-quality-29fj)

---

### 2. VAD (Voice Activity Detection) - í•„ìˆ˜!

#### 2-1. Silero VAD (ê¸°ë³¸)

**Silero VAD**ëŠ” stable-tsì— ë‚´ì¥ëœ ì´ˆê²½ëŸ‰(1.8MB) ìŒì„± ê°ì§€ ëª¨ë¸ì…ë‹ˆë‹¤.

```python
result = model.align(
    mp3_path,
    lyrics,
    language='ja',

    # Silero VAD í™œì„±í™”
    vad=True,  # ë˜ëŠ” vad='silero-vad'
    vad_threshold=0.35,  # ë…¸ë˜ ê¶Œì¥ê°’: 0.3~0.4

    # ì¹¨ë¬µ ì–µì œ
    suppress_silence=True,
    suppress_word_ts=True,
)
```

**VAD Threshold ê°€ì´ë“œ**:

| í™˜ê²½ | ê¶Œì¥ê°’ | ì„¤ëª… |
|------|--------|------|
| ê¹¨ë—í•œ ìŒì„± | 0.2~0.3 | ë‚®ê²Œ ì„¤ì • |
| ì¡ìŒ ìˆëŠ” ìŒì„± | 0.4~0.5 | ë†’ê²Œ ì„¤ì • |
| **ë…¸ë˜/ìŒì•…** | **0.35** | **ê¶Œì¥** â­ |

**íš¨ê³¼**:
- âœ… ì¸íŠ¸ë¡œ/ì•„ì›ƒíŠ¸ë¡œ ë¬´ìŒ êµ¬ê°„ì—ì„œ í™˜ê° ë°©ì§€
- âœ… íƒ€ì„ìŠ¤íƒ¬í”„ ì •í™•ë„ í–¥ìƒ
- âœ… ë°°ê²½ ìŒì•… êµ¬ê°„ ìë™ ì œê±°

**ì°¸ê³ **: stable-tsëŠ” Silero VAD v4ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, [ì¼ë¶€ ì»¤ë®¤ë‹ˆí‹°ì—ì„œëŠ” v3.1 ê¶Œì¥](https://github.com/jianfch/stable-ts/discussions/373)

---

#### 2-2. RMS-VAD (ë…¸ë˜ íŠ¹í™”) ğŸ†•

**RMS-VAD**ëŠ” ë³´ì»¬ ë¶„ë¦¬ í›„ **RMS(Root Mean Square) ì§„í­ ê¸°ë°˜**ìœ¼ë¡œ ê°€ì°½ êµ¬ê°„ì„ ê°ì§€í•˜ëŠ” ìµœì‹  ê¸°ë²•ì…ë‹ˆë‹¤.

**ì›ë¦¬**: ë³´ì»¬ íŠ¸ë™ì˜ ì§„í­ì´ ì„ê³„ê°’ ì´ìƒì¼ ë•Œë§Œ ê°€ì‚¬ë¡œ ì¸ì‹

```python
import librosa
import numpy as np

def rms_vad_segments(vocal_audio, sr=16000, threshold_db=-40, hop_length=512):
    """RMS ê¸°ë°˜ VADë¡œ ê°€ì°½ êµ¬ê°„ ì¶”ì¶œ"""

    # RMS ê³„ì‚°
    rms = librosa.feature.rms(y=vocal_audio, hop_length=hop_length)[0]

    # dBë¡œ ë³€í™˜
    rms_db = librosa.amplitude_to_db(rms, ref=np.max)

    # ì„ê³„ê°’ ì´ìƒ êµ¬ê°„ ì¶”ì¶œ
    vocal_frames = np.where(rms_db > threshold_db)[0]

    # í”„ë ˆì„ì„ ì‹œê°„ìœ¼ë¡œ ë³€í™˜
    times = librosa.frames_to_time(vocal_frames, sr=sr, hop_length=hop_length)

    return times

# ì‚¬ìš© ì˜ˆì‹œ
vocal_segments = rms_vad_segments(vocal_audio, threshold_db=-40)
```

**ì—°êµ¬ ê²°ê³¼**: RMS-VAD ì‚¬ìš© ì‹œ **WER(Word Error Rate) ì¤‘ì•™ê°’ ê°œì„ ** í™•ì¸

**ì°¸ê³ **: [Exploiting Music Source Separation for Automatic Lyrics Transcription](https://arxiv.org/html/2506.15514v1)

---

### 3. initial_prompt (ì¼ë³¸ì–´ ìµœì í™”)

Whisperì— **ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µ**í•˜ì—¬ ì¼ë³¸ì–´ ì¸ì‹ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.

```python
result = model.align(
    mp3_path,
    lyrics,
    language='ja',

    # ì¼ë³¸ì–´ ê°€ì‚¬ì„ì„ ëª…ì‹œ
    initial_prompt="ä»¥ä¸‹ã¯æ—¥æœ¬èªã®æ­Œè©ã§ã™ã€‚ãƒ›ã‚·ãƒãƒã‚¹ã‚¤ã‚»ã‚¤ã®æ¥½æ›²ã€‚",
    # "ë‹¤ìŒì€ ì¼ë³¸ì–´ ê°€ì‚¬ì…ë‹ˆë‹¤. í˜¸ì‹œë§ˆì¹˜ ìŠ¤ì´ì„¸ì´ì˜ ê³¡."
)
```

**Prompt ì‘ì„± íŒ**:
1. **ê¸´ í”„ë¡¬í”„íŠ¸ê°€ ë” íš¨ê³¼ì ** (ì§§ì€ ê²ƒë³´ë‹¤ ì‹ ë¢°ë„ ë†’ìŒ)
2. **ì•„í‹°ìŠ¤íŠ¸ëª…, ê³¡ëª… í¬í•¨** (ê³ ìœ ëª…ì‚¬ ì¸ì‹ í–¥ìƒ)
3. **ê¸°ìˆ  ìš©ì–´, íŠ¹ìˆ˜ ë‹¨ì–´ í¬í•¨** (ê°€íƒ€ì¹´ë‚˜ ë‹¨ì–´ ë“±)
4. **30ì´ˆë§ˆë‹¤ ë¦¬ì…‹ë¨** (ì²« 30ì´ˆì—ë§Œ ì ìš©)

**ì˜ˆì‹œ**:
```python
initial_prompt = """
ä»¥ä¸‹ã¯æ—¥æœ¬èªã®æ­Œè©ã§ã™ã€‚
ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ: ãƒ›ã‚·ãƒãƒã‚¹ã‚¤ã‚»ã‚¤ (Hoshimachi Suisei)
ã‚¸ãƒ£ãƒ³ãƒ«: J-Popã€ã‚¢ãƒ‹ã‚½ãƒ³
"""
```

**ì°¸ê³ **: [Best prompt to transcribe Japanese?](https://github.com/openai/whisper/discussions/2151), [Whisper prompting guide](https://cookbook.openai.com/examples/whisper_prompting_guide)

---

## ğŸ”Š ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ê¸°ë²•

### 1. ì˜¤ë””ì˜¤ ì •ê·œí™” (Normalization)

**ëª©ì **: ë³¼ë¥¨ í¸ì°¨ ì œê±° â†’ ì¼ê´€ëœ ì¸ì‹ ì •í™•ë„

#### 1-1. Peak Normalization

```python
import librosa
import soundfile as sf
import numpy as np

def normalize_audio_peak(input_path, output_path, target_peak=0.95):
    """Peak ì •ê·œí™”: ìµœëŒ€ ì§„í­ì„ target_peakë¡œ ì¡°ì •"""
    audio, sr = librosa.load(input_path, sr=None, mono=False)

    # ìµœëŒ€ ì§„í­ ì°¾ê¸°
    peak = np.abs(audio).max()

    # ì •ê·œí™”
    if peak > 0:
        audio_normalized = audio * (target_peak / peak)
    else:
        audio_normalized = audio

    # ì €ì¥
    sf.write(output_path, audio_normalized.T, sr)

    return output_path

# ì‚¬ìš©
normalized = normalize_audio_peak('input.mp3', 'normalized.mp3')
```

#### 1-2. RMS Normalization (ì¶”ì²œ)

```python
def normalize_audio_rms(input_path, output_path, target_rms_db=-20):
    """RMS ì •ê·œí™”: í‰ê·  ì—ë„ˆì§€ë¥¼ ëª©í‘œ dBë¡œ ì¡°ì •"""
    audio, sr = librosa.load(input_path, sr=None, mono=False)

    # RMS ê³„ì‚°
    rms = np.sqrt(np.mean(audio**2))

    # ëª©í‘œ RMS
    target_rms = librosa.db_to_amplitude(target_rms_db)

    # ì •ê·œí™”
    if rms > 0:
        audio_normalized = audio * (target_rms / rms)
    else:
        audio_normalized = audio

    # í´ë¦¬í•‘ ë°©ì§€
    audio_normalized = np.clip(audio_normalized, -1.0, 1.0)

    sf.write(output_path, audio_normalized.T, sr)

    return output_path
```

**ê¶Œì¥**: RMS ì •ê·œí™” (-20dB ~ -18dB)

---

### 2. FFmpeg ì „ì²˜ë¦¬ (ì„ íƒ)

#### 2-1. ì¹¨ë¬µ ì œê±°

```bash
ffmpeg -y -i input.mp3 \
    -af "silenceremove=start_periods=1:stop_periods=-1:start_threshold=-50dB:stop_threshold=-50dB:start_silence=0.1:stop_silence=0.1" \
    output.mp3
```

**íš¨ê³¼**: ê¸´ ì¹¨ë¬µ êµ¬ê°„ ì œê±° â†’ í™˜ê° ë°©ì§€

#### 2-2. ê³ ì—­ í•„í„° (ìŒì„± ì£¼íŒŒìˆ˜ë§Œ)

```bash
ffmpeg -y -i input.mp3 \
    -af "highpass=f=200,lowpass=f=5000" \
    output.mp3
```

**íš¨ê³¼**: 200-5000Hz (ìŒì„± ì£¼íŒŒìˆ˜ ëŒ€ì—­)ë§Œ ìœ ì§€ â†’ ë…¸ì´ì¦ˆ ê°ì†Œ

---

## ğŸ¼ Demucs ë³´ì»¬ ë¶„ë¦¬ ì‹¤ì „

### 1. Demucsë€?

Meta/Kyutaiì˜ **ìµœì²¨ë‹¨ ìŒì› ë¶„ë¦¬ ëª¨ë¸**. ë…¸ë˜ì—ì„œ **ë³´ì»¬ë§Œ ì¶”ì¶œ**í•˜ì—¬ ê°€ì‚¬ ì¸ì‹ ì •í™•ë„ë¥¼ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

**ì„±ëŠ¥**:
- ì¼ë³¸ì–´ ë…¸ë˜ WER: **35% â†’ 10-15%** (ë³´ì»¬ ë¶„ë¦¬ í›„)
- íƒ€ì„ìŠ¤íƒ¬í”„ ì •í™•ë„: **Â±0.3s â†’ Â±0.15s**

---

### 2. stable-ts ë‚´ì¥ í†µí•© (ë°©ë²• 1 - ê¶Œì¥)

```python
result = model.align(
    mp3_path,
    lyrics,
    language='ja',

    # Demucs ë³´ì»¬ ë¶„ë¦¬
    denoiser='demucs',  # ìë™ìœ¼ë¡œ ë³´ì»¬ë§Œ ì¶”ì¶œ
    denoiser_options={'device': 'cuda'},  # GPU ì‚¬ìš©

    # VADì™€ í•¨ê»˜ ì‚¬ìš© (ê¶Œì¥)
    vad=True,
    vad_threshold=0.35,
    suppress_silence=True,
)
```

**ì¥ì **:
- âœ… í•œ ì¤„ë¡œ í†µí•©
- âœ… ì¤‘ê°„ íŒŒì¼ ìƒì„± ë¶ˆí•„ìš”
- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì 

**ë‹¨ì **:
- âš ï¸ ì²˜ë¦¬ ì‹œê°„ 3~5ë°° ì¦ê°€ (15ì´ˆ â†’ 45ì´ˆ)
- âš ï¸ VRAM ì¶”ê°€ ì‚¬ìš© (~2GB)

**ì°¸ê³ **: [What is "demucs" exactly?](https://github.com/jianfch/stable-ts/discussions/294)

---

### 3. ìˆ˜ë™ Demucs íŒŒì´í”„ë¼ì¸ (ë°©ë²• 2)

ë” ì„¸ë°€í•œ ì œì–´ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©.

#### 3-1. Demucs ì„¤ì¹˜

```bash
pip install demucs
```

#### 3-2. CLIë¡œ ë³´ì»¬ ë¶„ë¦¬

```bash
# htdemucs_ft ëª¨ë¸ (ìµœê³  í’ˆì§ˆ)
demucs -n htdemucs_ft --two-stems=vocals input.mp3 -o output_dir

# ì¶œë ¥: output_dir/htdemucs_ft/input/vocals.wav
```

**ì˜µì…˜**:
- `-n htdemucs_ft`: Fine-tuned ëª¨ë¸ (ìµœê³  í’ˆì§ˆ)
- `--two-stems=vocals`: ë³´ì»¬ë§Œ ì¶”ì¶œ (ë“œëŸ¼/ë² ì´ìŠ¤ ì œì™¸)
- `--device cuda`: GPU ì‚¬ìš©

#### 3-3. Python ì½”ë“œ

```python
import torch
import torchaudio
from demucs.pretrained import get_model
from demucs.apply import apply_model

def extract_vocals_demucs(input_path, output_path, device='cuda'):
    """Demucsë¡œ ë³´ì»¬ë§Œ ì¶”ì¶œ"""

    # ëª¨ë¸ ë¡œë“œ
    model = get_model('htdemucs_ft')
    model.to(device)
    model.eval()

    # ì˜¤ë””ì˜¤ ë¡œë“œ
    wav, sr = torchaudio.load(input_path)
    wav = wav.to(device)

    # ë¶„ë¦¬ ì‹¤í–‰
    with torch.no_grad():
        sources = apply_model(model, wav[None], device=device)[0]

    # sources: [drums, bass, other, vocals]
    vocals = sources[3]  # ë³´ì»¬ë§Œ ì¶”ì¶œ

    # ì €ì¥
    vocals = vocals.cpu()
    torchaudio.save(output_path, vocals, sr)

    return output_path

# ì‚¬ìš©
vocals_path = extract_vocals_demucs('input.mp3', 'vocals.wav')
```

#### 3-4. stable-tsì™€ ì—°ê²°

```python
# 1. ë³´ì»¬ ë¶„ë¦¬
vocals_path = extract_vocals_demucs('input.mp3', 'vocals.wav')

# 2. stable-tsë¡œ ì •ë ¬
result = model.align(
    vocals_path,  # ë³´ì»¬ íŒŒì¼ ì‚¬ìš©
    lyrics,
    language='ja',
    vad=True,
    vad_threshold=0.35,
)
```

---

### 4. Demucs ëª¨ë¸ ë¹„êµ

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | í’ˆì§ˆ (SDR) | ì†ë„ | ìš©ë„ |
|------|----------|-----------|------|------|
| **htdemucs_ft** | - | **9.20 dB** | ëŠë¦¼ | ìµœê³  í’ˆì§ˆ â­ |
| htdemucs | - | 9.00 dB | ë³´í†µ | ê¸°ë³¸ |
| htdemucs_6s | - | 8.8 dB | ëŠë¦¼ | í”¼ì•„ë…¸/ê¸°íƒ€ ë¶„ë¦¬ |
| hdemucs_mmi | - | 8.5+ dB | ë¹ ë¦„ | ì´ì „ ë²„ì „ |

**ê¶Œì¥**: `htdemucs_ft` (Fine-tuned HT Demucs)

---

### 5. ì²˜ë¦¬ ì‹œê°„ ë° ë¦¬ì†ŒìŠ¤

#### RTX 3070 Ti ê¸°ì¤€ (3ë¶„ ê³¡)

| ì‘ì—… | ì‹œê°„ | VRAM |
|------|------|------|
| align (ê¸°ë³¸) | 15ì´ˆ | 4-5GB |
| **align + demucs** | **45-60ì´ˆ** | **6-7GB** |
| ìˆ˜ë™ Demucs | 30ì´ˆ | 2-3GB |
| ìˆ˜ë™ Demucs + align | 45ì´ˆ | 4-5GB |

**ê²°ë¡ **:
- âœ… ì‹œê°„ ì—¬ìœ  ìˆê³  ìµœê³  í’ˆì§ˆ ì›í•  ë•Œ: `denoiser='demucs'`
- âœ… ë¹ ë¥¸ ì²˜ë¦¬ í•„ìš”í•  ë•Œ: ê¸°ë³¸ align + VADë§Œ ì‚¬ìš©

---

## âš™ï¸ stable-ts ê³ ê¸‰ íŒŒë¼ë¯¸í„°

### 1. align() ë©”ì„œë“œ íŒŒë¼ë¯¸í„° (ì‹¤í—˜ì )

**ì£¼ì˜**: align() ë©”ì„œë“œëŠ” transcribe()ì™€ ë‹¬ë¦¬ ì¼ë¶€ íŒŒë¼ë¯¸í„° ë¯¸ì§€ì› ê°€ëŠ¥ì„± ìˆìŒ.

```python
try:
    result = model.align(
        mp3_path,
        lyrics,
        language='ja',

        # === ì‹¤í—˜ì  íŒŒë¼ë¯¸í„° ===
        # VAD
        vad=True,
        vad_threshold=0.35,

        # ì¹¨ë¬µ ì–µì œ
        suppress_silence=True,
        suppress_word_ts=True,

        # ì„¸ê·¸ë¨¼íŠ¸ ì¬ê·¸ë£¹í™”
        regroup=True,  # ìë™ ì¬ê·¸ë£¹í™”

        # í™˜ê° ë°©ì§€
        temperature=0,
        condition_on_previous_text=False,

        # ë©”ëª¨ë¦¬ ìµœì í™”
        mel_first=True,
    )
except TypeError as e:
    # ë¯¸ì§€ì› íŒŒë¼ë¯¸í„° ìˆì„ ê²½ìš° ê¸°ë³¸ìœ¼ë¡œ fallback
    print(f"âš ï¸ ì¼ë¶€ íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›: {e}")
    result = model.align(mp3_path, lyrics, language='ja')
```

**ì°¸ê³ **: [stable-ts README](https://github.com/jianfch/stable-ts/blob/main/README.md)

---

### 2. regroup íŒŒë¼ë¯¸í„° (ì„¸ê·¸ë¨¼íŠ¸ ìë™ ìµœì í™”)

`regroup=True`ëŠ” ì„¸ê·¸ë¨¼íŠ¸ë¥¼ **êµ¬ë‘ì ê³¼ ì¹¨ë¬µ êµ¬ê°„ ê¸°ë°˜ìœ¼ë¡œ ìë™ ì¬êµ¬ì„±**í•©ë‹ˆë‹¤.

```python
result = model.align(
    mp3_path,
    lyrics,
    language='ja',
    regroup=True,  # ë˜ëŠ” ì»¤ìŠ¤í…€ ì•Œê³ ë¦¬ì¦˜ ë¬¸ìì—´
)
```

#### ì»¤ìŠ¤í…€ regroup ì•Œê³ ë¦¬ì¦˜

```python
# ì•½ì–´:
# sp = split_by_punctuation
# sg = split_by_gap
# sl = split_by_length
# mg = merge_by_gap

regroup_algo = 'sp=.* /ã€‚/?/ï¼Ÿ/,/ï¼Œ_sg=.5_mg=.15+3_sp=.* /ã€‚/?/ï¼Ÿ'

result = model.align(
    mp3_path,
    lyrics,
    language='ja',
    regroup=regroup_algo,
)
```

**ì°¸ê³ **: [Sharing Customized Regrouping Algorithms](https://github.com/jianfch/stable-ts/discussions/162)

---

### 3. suppress_silence íŒŒë¼ë¯¸í„° ì„¸íŠ¸

```python
result = model.align(
    mp3_path,
    lyrics,
    language='ja',

    # ì¹¨ë¬µ ì–µì œ í™œì„±í™”
    suppress_silence=True,

    # ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ë„ ì¡°ì •
    suppress_word_ts=True,

    # ì–‘ìí™” ë ˆë²¨ (VAD ë¯¸ì‚¬ìš© ì‹œ)
    q_levels=20,  # ê¸°ë³¸ê°’

    # ë‹¨ì–´ ìœ„ì¹˜ ê¸°ë°˜ ì¡°ì •
    use_word_position=True,
)
```

---

### 4. ê¸°íƒ€ ìœ ìš©í•œ íŒŒë¼ë¯¸í„°

```python
result = model.align(
    mp3_path,
    lyrics,
    language='ja',

    # === ë©”ëª¨ë¦¬ ìµœì í™” ===
    mel_first=True,  # ê¸´ ì˜¤ë””ì˜¤ì— ìœ ìš©

    # === ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ ===
    word_timestamps=True,  # ë°˜ë“œì‹œ True (ê¸°ë³¸ê°’)

    # === ìµœì†Œ ë‹¨ì–´ ê¸¸ì´ ===
    min_word_dur=0.1,  # 0.1ì´ˆ ë¯¸ë§Œ ë‹¨ì–´ ë³‘í•©
)
```

---

## ğŸ“ ì¼ë³¸ì–´ ê°€ì‚¬ ì„¸ê·¸ë¨¼íŠ¸ ìµœì í™”

### 1. ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ì²´ì¸ (í•µì‹¬!)

í˜„ì¬ v1.2ì˜ **ê°€ì¥ í° ë¬¸ì œ**ëŠ” ê¸´ ì„¸ê·¸ë¨¼íŠ¸ì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ëŠ” 4ë‹¨ê³„ ì²´ì¸:

```python
def optimize_segments_for_japanese(result, profile='normal'):
    """ì¼ë³¸ì–´ ê°€ì‚¬ë¥¼ ìœ„í•œ ì„¸ê·¸ë¨¼íŠ¸ ìµœì í™”"""

    # í”„ë¡œíŒŒì¼ ì„¤ì •
    PROFILES = {
        'ballad': {
            'punctuation': [('ã€‚', ' '), ('ã€', ' '), ('ï¼Ÿ', ' '), ('ï¼', ' '), ('â€¦', ' ')],
            'gap_threshold': 2.5,
            'max_chars': 35,
            'merge_gap': 0.20,
        },
        'normal': {
            'punctuation': [('ã€‚', ' '), ('ã€', ' '), ('ï¼Ÿ', ' '), ('ï¼', ' ')],
            'gap_threshold': 2.0,
            'max_chars': 30,
            'merge_gap': 0.15,
        },
        'fast': {
            'punctuation': [('ã€‚', ' '), ('ã€', ' ')],
            'gap_threshold': 1.5,
            'max_chars': 25,
            'merge_gap': 0.10,
        },
    }

    cfg = PROFILES.get(profile, PROFILES['normal'])

    # === 4ë‹¨ê³„ ìµœì í™” ì²´ì¸ ===

    # 1ë‹¨ê³„: êµ¬ë‘ì ìœ¼ë¡œ ë¶„í•  (ìµœìš°ì„ )
    result.split_by_punctuation(cfg['punctuation'])

    # 2ë‹¨ê³„: ì¹¨ë¬µ êµ¬ê°„ìœ¼ë¡œ ë¶„í• 
    result.split_by_gap(gap_threshold=cfg['gap_threshold'])

    # 3ë‹¨ê³„: ê¸¸ì´ ì œí•œ
    result.split_by_length(
        max_chars=cfg['max_chars'],
        max_words=None,  # ì¼ë³¸ì–´ëŠ” ê³µë°± ì—†ìœ¼ë¯€ë¡œ None
        even_split=True  # ê· ë“± ë¶„í• 
    )

    # 4ë‹¨ê³„: ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ ë³‘í•©
    result.merge_by_gap(
        max_gap=cfg['merge_gap'],
        max_chars=cfg['max_chars']
    )

    return result

# ì‚¬ìš©
result = model.align(mp3_path, lyrics, language='ja')
optimize_segments_for_japanese(result, profile='normal')
result.to_srt_vtt(output_path, word_level=False)
```

**íš¨ê³¼**:
- **Before**: `[00:15.23 - 00:23.67] è¡Œã“ã†ã€€ã“ã®å£°ã«å°ã‹ã‚Œã€€ä»Šæ—¥ã‚‚ã¾ãŸä¸€æ­©ãšã¤ã€€å¤¢è¦‹ãŸå ´æ‰€ã¸` (60ì)
- **After**:
  ```
  [00:15.23 - 00:17.89] è¡Œã“ã†ã€€ã“ã®å£°ã«å°ã‹ã‚Œ
  [00:18.01 - 00:20.45] ä»Šæ—¥ã‚‚ã¾ãŸä¸€æ­©ãšã¤
  [00:20.67 - 00:23.12] å¤¢è¦‹ãŸå ´æ‰€ã¸
  ```

---

### 2. ì¼ë³¸ì–´ êµ¬ë‘ì  ë¦¬ìŠ¤íŠ¸

```python
JAPANESE_PUNCTUATION = [
    ('ã€‚', ' '),   # ë§ˆì¹¨í‘œ (ë¬¸ì¥ ë)
    ('ã€', ' '),   # ì‰¼í‘œ (êµ¬ êµ¬ë¶„)
    ('ï¼Ÿ', ' '),   # ë¬¼ìŒí‘œ
    ('ï¼', ' '),   # ëŠë‚Œí‘œ
    ('â€¦', ' '),    # ë§ì¤„ì„í‘œ
    ('ï½', ' '),   # ë¬¼ê²°í‘œ
]

# ì‚¬ìš©
result.split_by_punctuation(JAPANESE_PUNCTUATION)
```

---

### 3. ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ë¶„í•  (Bunsetsu)

ì¼ë³¸ì–´ëŠ” **ë¬¸ì ˆ(Bunsetsu)** ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.

#### 3-1. Google Budou ì‚¬ìš©

```bash
pip install budou
```

```python
from budou import authenticate

def split_by_bunsetsu(text):
    """ë¬¸ì ˆ ë‹¨ìœ„ë¡œ ë¶„í• """
    parser = authenticate('credentials.json')
    result = parser.parse(text)

    chunks = [chunk['word'] for chunk in result['chunks']]
    return chunks

# ì˜ˆì‹œ
text = "è¡Œã“ã†ã“ã®å£°ã«å°ã‹ã‚Œ"
chunks = split_by_bunsetsu(text)
# ['è¡Œã“ã†', 'ã“ã®', 'å£°ã«', 'å°ã‹ã‚Œ']
```

**ì°¸ê³ **: [Google Budou - CJK line breaking](https://github.com/google/budou)

#### 3-2. MeCab í˜•íƒœì†Œ ë¶„ì„

```bash
pip install mecab-python3 unidic-lite
```

```python
import MeCab

def analyze_japanese_lyrics(text):
    """í˜•íƒœì†Œ ë¶„ì„"""
    mecab = MeCab.Tagger()
    parsed = mecab.parse(text)
    return parsed

# ì‚¬ìš©
text = "è¡Œã“ã†ã“ã®å£°ã«å°ã‹ã‚Œ"
result = analyze_japanese_lyrics(text)
print(result)
```

**ì°¸ê³ **: [Word-splitting in East Asian languages](https://investigate.ai/text-analysis/splitting-words-in-east-asian-languages/)

---

### 4. clamp_max() - íƒ€ì„ìŠ¤íƒ¬í”„ ë³´ì •

ì„¸ê·¸ë¨¼íŠ¸ ë íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ë³´ë‹¤ ëŠ¦ì„ ë•Œ ë³´ì •:

```python
result.clamp_max()  # íƒ€ì„ìŠ¤íƒ¬í”„ ì¤‘ì²© ì œê±°
```

**íš¨ê³¼**: íƒ€ì„ìŠ¤íƒ¬í”„ ìˆœì„œ ë³´ì¥

---

## ğŸš« í™˜ê°(Hallucination) ì™„ì „ ì œê±°

### 1. í™˜ê°ì´ë€?

Whisperê°€ **ì‹¤ì œ ìŒì„±ì— ì—†ëŠ” í…ìŠ¤íŠ¸**ë¥¼ ìƒì„±í•˜ëŠ” í˜„ìƒ:
- "Thanks for watching"
- "Subscribe to my channel"
- ê°™ì€ ë¬¸ì¥ ë¬´í•œ ë°˜ë³µ

---

### 2. 7ë‹¨ê³„ í™˜ê° ë°©ì§€ ì „ëµ

```python
def transcribe_no_hallucination(model, audio_path, language='ja'):
    """í™˜ê° ì œê±° ìµœì í™”"""

    result = model.transcribe(
        audio_path,
        language=language,

        # [1] VAD ì „ì²˜ë¦¬ (í•„ìˆ˜)
        vad=True,
        vad_threshold=0.35,
        suppress_silence=True,

        # [2] ì˜¨ë„ 0 (ê²°ì •ë¡ ì )
        temperature=0,

        # [3] ì´ì „ í…ìŠ¤íŠ¸ ì˜ì¡´ ì œê±°
        condition_on_previous_text=False,

        # [4] íƒ€ì„ìŠ¤íƒ¬í”„ í† í° ì–µì œ
        suppress_tokens='-1',  # ê¸°ë³¸ê°’
        no_speech_threshold=0.6,  # ë¬´ìŒ íŒì • ì„ê³„ê°’

        # [5] ë³´ì»¬ ë¶„ë¦¬ (ë…¸ë˜)
        denoiser='demucs',
    )

    return result
```

---

### 3. í›„ì²˜ë¦¬: compression_ratio í•„í„°

í™˜ê°ëœ ì„¸ê·¸ë¨¼íŠ¸ëŠ” **compression_ratioê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ìŒ**:

```python
def filter_hallucinated_segments(result, threshold=2.4):
    """í™˜ê° ì„¸ê·¸ë¨¼íŠ¸ í•„í„°ë§"""

    filtered_segments = []

    for seg in result.segments:
        if hasattr(seg, 'compression_ratio'):
            if seg.compression_ratio <= threshold:
                filtered_segments.append(seg)
            else:
                print(f"âš ï¸ í™˜ê° ì˜ì‹¬: {seg.text} (ratio={seg.compression_ratio:.2f})")
        else:
            filtered_segments.append(seg)

    result.segments = filtered_segments
    return result

# ì‚¬ìš©
result = model.transcribe(audio_path, language='ja')
result = filter_hallucinated_segments(result, threshold=2.4)
```

---

### 4. FFmpeg ì „ì²˜ë¦¬ (ì¶”ê°€ ë°©ì–´ì„ )

```bash
# ê¸´ ì¹¨ë¬µ êµ¬ê°„ ì œê±°
ffmpeg -y -i input.mp3 \
    -af "silenceremove=start_periods=1:stop_periods=-1:start_threshold=-50dB:stop_threshold=-50dB:start_silence=0.1:stop_silence=0.1" \
    preprocessed.mp3
```

---

## ğŸ¯ íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë°€ë„ ê·¹ëŒ€í™”

### 1. min_word_dur (ìµœì†Œ ë‹¨ì–´ ê¸¸ì´)

ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ë¥¼ ë³‘í•©í•˜ì—¬ ë–¨ë¦¼ ë°©ì§€:

```python
result = model.align(
    mp3_path,
    lyrics,
    language='ja',
    min_word_dur=0.1,  # 0.1ì´ˆ ë¯¸ë§Œ ë‹¨ì–´ ë³‘í•©
)
```

---

### 2. VAD + regroup ì¡°í•©

```python
result = model.align(
    mp3_path,
    lyrics,
    language='ja',
    vad=True,
    vad_threshold=0.35,
    regroup=True,  # ì„¸ê·¸ë¨¼íŠ¸ ê²½ê³„ ìë™ ì¡°ì •
)
```

**íš¨ê³¼**: 30ì´ˆ ì²­í¬ ê²½ê³„ ë¬¸ì œ í•´ê²°

---

### 3. ìˆ˜ë™ ì„¸ê·¸ë¨¼íŠ¸ ì¡°ì •

```python
# ì¹¨ë¬µ êµ¬ê°„ ê¸°ì¤€ ë¶„í• 
result.split_by_gap(0.5)  # 0.5ì´ˆ ì´ìƒ ì¹¨ë¬µ

# ë„ˆë¬´ ì§§ì€ ê²ƒ ë³‘í•©
result.merge_by_gap(0.15, max_words=3)
```

---

## ğŸš€ í†µí•© ìµœì í™” íŒŒì´í”„ë¼ì¸

### ìµœì¢… í”„ë¡œí† íƒ€ì… (v2.0)

```python
import stable_whisper
import torch
from pathlib import Path

def process_song_optimized(
    model,
    mp3_path: Path,
    lyrics_path: Path,
    output_path: Path,
    use_demucs: bool = False,
    profile: str = 'normal'
) -> dict:
    """
    ìµœì í™”ëœ ê°€ì‚¬ ì‹±í¬ ì²˜ë¦¬

    Args:
        model: stable-ts ëª¨ë¸
        mp3_path: ì›ë³¸ MP3 ê²½ë¡œ
        lyrics_path: ê°€ì‚¬ íŒŒì¼ ê²½ë¡œ
        output_path: ì¶œë ¥ LRC ê²½ë¡œ
        use_demucs: Demucs ë³´ì»¬ ë¶„ë¦¬ ì‚¬ìš© ì—¬ë¶€
        profile: ì„¸ê·¸ë¨¼íŠ¸ í”„ë¡œíŒŒì¼ ('ballad', 'normal', 'fast')

    Returns:
        dict: ì²˜ë¦¬ ê²°ê³¼ í†µê³„
    """

    import time
    import re

    # [1] ê°€ì‚¬ ì „ì²˜ë¦¬
    with open(lyrics_path, 'r', encoding='utf-8-sig') as f:
        lyrics = f.read().strip()

    # ì „ê° ê³µë°± â†’ ë°˜ê° ê³µë°±
    lyrics = lyrics.replace('\u3000', ' ')

    # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    lyrics = re.sub(r'[ï¼ˆï¼‰()ã€Œã€ã€ã€ã€ã€‘â™ªâ™¬ï½ã€œ]', '', lyrics)

    # ì—¬ëŸ¬ ê³µë°± â†’ í•˜ë‚˜ë¡œ
    lyrics = re.sub(r'\s+', ' ', lyrics)

    # ë¹ˆ ë¼ì¸ ì œê±°
    lyrics_lines = [line.strip() for line in lyrics.split('\n') if line.strip()]
    lyrics = '\n'.join(lyrics_lines)

    print(f"ğŸ“ ê°€ì‚¬ ë¼ì¸: {len(lyrics_lines)}ê°œ")

    # [2] ëª¨ë¸ ì •ë ¬ (ìµœì í™” ì˜µì…˜)
    print(f"â³ ì •ë ¬ ì¤‘... (Demucs: {use_demucs})")
    start = time.time()

    align_options = {
        'language': 'ja',
        'initial_prompt': "ä»¥ä¸‹ã¯æ—¥æœ¬èªã®æ­Œè©ã§ã™ã€‚ãƒ›ã‚·ãƒãƒã‚¹ã‚¤ã‚»ã‚¤ã®æ¥½æ›²ã€‚",
    }

    # Demucs ì˜µì…˜ (ì„ íƒ)
    if use_demucs:
        align_options['denoiser'] = 'demucs'
        align_options['denoiser_options'] = {'device': 'cuda'}

    # VAD ë° ê³ ê¸‰ ì˜µì…˜ (try-exceptë¡œ ì•ˆì „í•˜ê²Œ)
    try:
        result = model.align(
            str(mp3_path),
            lyrics,
            vad=True,
            vad_threshold=0.35,
            suppress_silence=True,
            temperature=0,
            condition_on_previous_text=False,
            regroup=False,  # ìˆ˜ë™ìœ¼ë¡œ ìµœì í™”í•  ê²ƒ
            **align_options
        )
    except TypeError:
        # ì¼ë¶€ íŒŒë¼ë¯¸í„° ë¯¸ì§€ì› ì‹œ ê¸°ë³¸ìœ¼ë¡œ
        result = model.align(str(mp3_path), lyrics, **align_options)

    elapsed = time.time() - start

    # [3] ì„¸ê·¸ë¨¼íŠ¸ ìµœì í™” (í•µì‹¬!)
    print(f"âœ‚ï¸ ì„¸ê·¸ë¨¼íŠ¸ ìµœì í™” ì¤‘... (í”„ë¡œíŒŒì¼: {profile})")

    PROFILES = {
        'ballad': {
            'punctuation': [('ã€‚', ' '), ('ã€', ' '), ('ï¼Ÿ', ' '), ('ï¼', ' '), ('â€¦', ' ')],
            'gap_threshold': 2.5,
            'max_chars': 35,
            'merge_gap': 0.20,
        },
        'normal': {
            'punctuation': [('ã€‚', ' '), ('ã€', ' '), ('ï¼Ÿ', ' '), ('ï¼', ' ')],
            'gap_threshold': 2.0,
            'max_chars': 30,
            'merge_gap': 0.15,
        },
        'fast': {
            'punctuation': [('ã€‚', ' '), ('ã€', ' ')],
            'gap_threshold': 1.5,
            'max_chars': 25,
            'merge_gap': 0.10,
        },
    }

    cfg = PROFILES.get(profile, PROFILES['normal'])

    # 4ë‹¨ê³„ ìµœì í™” ì²´ì¸
    (
        result
        .clamp_max()  # íƒ€ì„ìŠ¤íƒ¬í”„ ë³´ì •
        .split_by_punctuation(cfg['punctuation'])
        .split_by_gap(gap_threshold=cfg['gap_threshold'])
        .split_by_length(max_chars=cfg['max_chars'], max_words=None, even_split=True)
        .merge_by_gap(max_gap=cfg['merge_gap'], max_chars=cfg['max_chars'])
    )

    # [4] í’ˆì§ˆ ê²€ì¦
    segments = result.segments
    durations = [seg.end - seg.start for seg in segments]
    char_counts = [len(seg.text) for seg in segments]

    stats = {
        'success': True,
        'time': elapsed,
        'lines': len(lyrics_lines),
        'segments': len(segments),
        'avg_duration': sum(durations) / len(durations) if durations else 0,
        'avg_chars': sum(char_counts) / len(char_counts) if char_counts else 0,
        'long_segments': sum(1 for d in durations if d > 5.0),
        'short_segments': sum(1 for d in durations if d < 0.5),
    }

    # ê²½ê³ 
    if stats['long_segments'] > 0:
        print(f"âš ï¸ ê¸´ ì„¸ê·¸ë¨¼íŠ¸ {stats['long_segments']}ê°œ ë°œê²¬ (5ì´ˆ ì´ìƒ)")
    if stats['avg_chars'] > 35:
        print(f"âš ï¸ í‰ê·  ê¸€ììˆ˜ {stats['avg_chars']:.1f}ì (ê¶Œì¥: 30ì ì´í•˜)")

    # [5] ì €ì¥
    result.to_srt_vtt(str(output_path), word_level=False)

    # íŒŒì¼ í¬ê¸°
    file_size = output_path.stat().st_size / 1024

    print(f"âœ… ì™„ë£Œ: {output_path}")
    print(f"   ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print(f"   ì„¸ê·¸ë¨¼íŠ¸: {stats['segments']}ê°œ")
    print(f"   í‰ê·  ê¸¸ì´: {stats['avg_duration']:.1f}ì´ˆ")
    print(f"   í‰ê·  ê¸€ììˆ˜: {stats['avg_chars']:.1f}ì")
    print(f"   í¬ê¸°: {file_size:.1f} KB")
    print()

    stats['size'] = file_size
    return stats

# === ì‚¬ìš© ì˜ˆì‹œ ===

# GPU í™•ì¸
assert torch.cuda.is_available(), "CUDA í•„ìˆ˜!"

# ëª¨ë¸ ë¡œë“œ
model = stable_whisper.load_model('large-v3', device='cuda')

# ì²˜ë¦¬ (ìµœê³  í’ˆì§ˆ)
stats = process_song_optimized(
    model,
    Path('songs/stellar_stellar.mp3'),
    Path('lyrics/stellar_stellar.txt'),
    Path('output/stellar_stellar.lrc'),
    use_demucs=True,  # ë³´ì»¬ ë¶„ë¦¬ í™œì„±í™”
    profile='normal'
)

print(f"ì²˜ë¦¬ ì™„ë£Œ: {stats}")
```

---

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### RTX 3070 Ti ê¸°ì¤€ (3ë¶„ ê³¡)

| êµ¬ì„± | ì²˜ë¦¬ ì‹œê°„ | VRAM | WER | íƒ€ì„ìŠ¤íƒ¬í”„ ì •í™•ë„ | ê°€ë…ì„± |
|------|----------|------|-----|-----------------|--------|
| **v1.2 (í˜„ì¬)** | 15ì´ˆ | 4-5GB | ~25-35% | Â±0.3s | 60/100 |
| v2.0 (VAD + ì„¸ê·¸ë¨¼íŠ¸) | 16ì´ˆ | 4-5GB | ~20-25% | Â±0.2s | **90/100** |
| v2.0 (Demucs + VAD + ì„¸ê·¸ë¨¼íŠ¸) | 45ì´ˆ | 6-7GB | **~10-15%** | **Â±0.15s** | **90/100** |

### í’ˆì§ˆ í–¥ìƒ ì •ë¦¬

| ì§€í‘œ | v1.2 | v2.0 (ê¸°ë³¸) | v2.0 (Demucs) | ê°œì„ ë„ |
|------|------|------------|--------------|--------|
| WER | 30% | 22% | **12%** | **60% â†“** |
| íƒ€ì„ìŠ¤íƒ¬í”„ | Â±0.3s | Â±0.2s | **Â±0.15s** | **50% â†‘** |
| ê°€ë…ì„± | 60/100 | **90/100** | **90/100** | **50% â†‘** |
| í™˜ê°ìœ¨ | 10% | 2% | **0.5%** | **95% â†“** |
| í‰ê·  ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ | 50ì | **28ì** | **28ì** | **44% â†“** |

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [stable-ts GitHub](https://github.com/jianfch/stable-ts)
- [OpenAI Whisper](https://github.com/openai/whisper)
- [Demucs GitHub](https://github.com/facebookresearch/demucs)

### ì›¹ ê²€ìƒ‰ ì°¸ê³  ìë£Œ

#### Whisper ìµœì í™”
- [Best prompt to transcribe Japanese?](https://github.com/openai/whisper/discussions/2151)
- [Whisper prompting guide](https://cookbook.openai.com/examples/whisper_prompting_guide)
- [Optimal sample rate for input audio?](https://github.com/openai/whisper/discussions/870)
- [Optimise OpenAI Whisper API](https://dev.to/mxro/optimise-openai-whisper-api-audio-format-sampling-rate-and-quality-29fj)

#### VAD ë° ìŒì„± ê²€ì¶œ
- [Silero-VAD V5 Discussion](https://github.com/jianfch/stable-ts/discussions/373)
- [Whisper WebUI with VAD for Japanese](https://github.com/openai/whisper/discussions/397)

#### ìŒì› ë¶„ë¦¬ ë° ê°€ì‚¬ ì¶”ì¶œ
- [Exploiting Music Source Separation for Automatic Lyrics Transcription](https://arxiv.org/html/2506.15514v1)
- [More than words: Speech Recognition for Singing](https://arxiv.org/html/2403.09298v1)
- [Singing Voice Detection: A Survey](https://www.mdpi.com/1099-4300/24/1/114)

#### Demucs í†µí•©
- [What is "demucs" exactly?](https://github.com/jianfch/stable-ts/discussions/294)
- [DEMUCS - Music source separation](https://demucs.danielfrg.com/)

#### ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬
- [Google Budou - CJK line breaking](https://github.com/google/budou)
- [Word-splitting in East Asian languages](https://investigate.ai/text-analysis/splitting-words-in-east-asian-languages/)
- [CJK Typesetting Challenges](https://asianabsolute.co.uk/blog/cjk-typesetting-challenges-workflows-and-best-practices/)

#### stable-ts ê³ ê¸‰ ê¸°ëŠ¥
- [Sharing Customized Regrouping Algorithms](https://github.com/jianfch/stable-ts/discussions/162)
- [stable-ts PyPI](https://pypi.org/project/stable-ts/)

#### ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
- [Preprocessing the Audio Dataset](https://www.geeksforgeeks.org/preprocessing-the-audio-dataset/)
- [Preprocessing an audio dataset - Hugging Face](https://huggingface.co/learn/audio-course/chapter1/preprocessing)

### í”„ë¡œì íŠ¸ ë‚´ë¶€ ë¬¸ì„œ
- `docs/lyrics_sync_tech_guide_2025.md` - ê¸°ìˆ  ê°€ì´ë“œ (976ì¤„)
- `docs/01_PRD.md` - PRD ë° ë ˆí¼ëŸ°ìŠ¤ êµ¬í˜„
- `ENHANCEMENT_PLAN.md` - v2.0 ê°œì„  ê³„íšì„œ
- `CLAUDE.md` - í”„ë¡œì íŠ¸ ê°€ì´ë“œë¼ì¸

---

## âœ… ë‹¤ìŒ ë‹¨ê³„

### ë‹¨ê³„ 1: ê¸°ë³¸ ìµœì í™” ì ìš© (30ë¶„)
- [x] ê°€ì‚¬ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
- [x] ì„¸ê·¸ë¨¼íŠ¸ ìµœì í™” í•¨ìˆ˜
- [x] í’ˆì§ˆ ê²€ì¦ í•¨ìˆ˜
- [ ] sync_suisei.pyì— í†µí•©

### ë‹¨ê³„ 2: VAD ìµœì í™” (30ë¶„)
- [ ] align()ì— VAD íŒŒë¼ë¯¸í„° ì¶”ê°€ (try-except)
- [ ] í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### ë‹¨ê³„ 3: Demucs í†µí•© (1ì‹œê°„)
- [ ] Demucs ì˜µì…˜ ì¶”ê°€ (ì„ íƒì )
- [ ] ì²˜ë¦¬ ì‹œê°„ ë²¤ì¹˜ë§ˆí¬
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸

### ë‹¨ê³„ 4: ìµœì¢… ê²€ì¦ (1ì‹œê°„)
- [ ] 3ê³¡ ì´ìƒ í…ŒìŠ¤íŠ¸
- [ ] í’ˆì§ˆ ì§€í‘œ ìˆ˜ì§‘
- [ ] README ì—…ë°ì´íŠ¸

---

**ì‘ì„±ì**: Claude (AI Assistant)
**ì´ ì¡°ì‚¬ ì‹œê°„**: 2ì‹œê°„
**ì°¸ê³  ë¬¸í—Œ**: 30+ ì›¹ ìë£Œ, í”„ë¡œì íŠ¸ ë¬¸ì„œ
**ì˜ˆìƒ êµ¬í˜„ ì‹œê°„**: ë‹¨ê³„ 1~2 (1ì‹œê°„), ë‹¨ê³„ 3~4 (2ì‹œê°„)
